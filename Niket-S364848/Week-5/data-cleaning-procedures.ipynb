{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42a14256-91c8-446e-92a7-ab6bf11055d3",
   "metadata": {},
   "source": [
    "## Why Data Cleaning is Important\n",
    "\n",
    "Data cleaning is a critical step in the data analysis process. It ensures that the data you are working with is accurate, consistent, and ready for analysis. Poor data quality can lead to incorrect conclusions, faulty models, and misguided business decisions. Therefore, it is essential to clean your data to eliminate errors, handle missing values, and ensure consistency before proceeding with any analysis.\n",
    "\n",
    "### Key Benefits of Data Cleaning:\n",
    "- **Accuracy**: Ensures that the data reflects the true values.\n",
    "- **Consistency**: Makes sure that the data follows a standard format across the dataset.\n",
    "- **Reliability**: Increases trust in the results derived from the data.\n",
    "- **Efficiency**: Reduces the time and effort required for data analysis and model building.\n",
    "\n",
    "\n",
    "## Common Data Cleaning Procedures with Python, NumPy, and pandas\n",
    "\n",
    "## 1. Handling Missing Values\n",
    "\n",
    "### a. Identifying missing values\n",
    "\n",
    "Why: Identifying missing values is the first and crucial step in data cleaning because missing data can lead to incorrect or biased results during analysis. It helps in understanding the extent of the issue and deciding on the appropriate strategy to handle it.\n",
    "\n",
    "When: This method is essential when we are dealing with datasets that might have incomplete information due to various reasons like data entry errors, sensor failures, or incomplete data collection processes.\n",
    "\n",
    "[1, 2, NaN, 4, NaN, 6]\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Check for missing values\n",
    "df.isnull().sum()\n",
    "\n",
    "# Visualize missing values\n",
    "import missingno as msno\n",
    "msno.matrix(df)\n",
    "```\n",
    "\n",
    "### b. Dropping missing values\n",
    "\n",
    "Why: Dropping missing values is useful when the missing data is minimal and does not significantly impact the dataset's integrity. It can help in simplifying the data processing pipeline, especially if imputing the missing data might introduce errors.\n",
    "\n",
    "When: This method is applied when the proportion of missing values is small, or when the missing data occurs randomly across a large dataset, making it feasible to remove affected rows or columns without losing significant information.\n",
    "\n",
    "```python\n",
    "# Drop rows with any missing values\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "# Drop columns with more than 50% missing values\n",
    "df_cleaned = df.dropna(thresh=len(df)*0.5, axis=1)\n",
    "```\n",
    "\n",
    "### c. Filling missing values\n",
    "\n",
    "Why: Filling missing values is a way to maintain the dataset's completeness without losing any data points, which is crucial for certain analyses like time series or when the dataset is small. Imputation methods such as filling with mean, median, or mode ensure that the imputed values represent the central tendency of the data, thus minimizing the bias.\n",
    "\n",
    "When: This method is used when the missing data is significant enough that dropping rows or columns would lead to loss of valuable information. Itâ€™s particularly important in scenarios where maintaining data continuity is necessary, such as in predictive modeling or when handling time-dependent data.\n",
    "\n",
    "\n",
    "```python\n",
    "# Fill with a specific value\n",
    "df['column'].fillna(0, inplace=True)\n",
    "\n",
    "# Fill with mean, median, or mode\n",
    "df['column'].fillna(df['column'].mean(), inplace=True)\n",
    "df['column'].fillna(df['column'].median(), inplace=True)\n",
    "df['column'].fillna(df['column'].mode()[0], inplace=True)\n",
    "\n",
    "# Forward fill\n",
    "df['column'].fillna(method='ffill', inplace=True)\n",
    "Original Data:\n",
    "[1, 2, NaN, 4, NaN, 6]\n",
    "\n",
    "After Forward Fill:\n",
    "[1, 2, 2, 4, 4, 6]\n",
    "\n",
    "\n",
    "# Backward fill\n",
    "df['column'].fillna(method='bfill', inplace=True)\n",
    "\n",
    "Original Data:\n",
    "[1, 2, NaN, 4, NaN, 6]\n",
    "\n",
    "After Backward Fill:\n",
    "[1, 2, 4, 4, 6, 6]\n",
    "```\n",
    "\n",
    "\n",
    "## 2. Handling Duplicates\n",
    "Why: Identifying duplicates is important because they can skew our analysis or lead to inaccurate results. Duplicate records might represent repeated measurements or data entry errors.\n",
    "\n",
    "When: This step is performed during the initial stages of data cleaning to ensure that your dataset does not contain redundant information.\n",
    "\n",
    "``` python\n",
    "# Identify duplicates\n",
    "duplicates = df[df.duplicated()]\n",
    "\n",
    "# Remove duplicates\n",
    "df_cleaned = df.drop_duplicates()\n",
    "\n",
    "# Remove duplicates, keeping the last occurrence\n",
    "df_cleaned = df.drop_duplicates(keep='last')\n",
    "```\n",
    "\n",
    "## 3. Data Type Conversion\n",
    "\n",
    "Why: Data type conversion ensures that data is in the correct format for analysis, calculations, or any operations you need to perform. Converting data types improves accuracy, allows for proper data handling, and optimizes performance by using the appropriate type for the task.\n",
    "\n",
    "When:\n",
    "\n",
    "Data is not in the expected format for analysis or operations.\n",
    "We need to perform specific operations (e.g., calculations, date comparisons) that require data to be in a particular format.\n",
    "Optimizing performance by reducing memory usage or speeding up data processing.\n",
    "\n",
    "``` python\n",
    "# Convert column to numeric\n",
    "df['column'] = pd.to_numeric(df['column'], errors='coerce')\n",
    "\n",
    "# Convert column to datetime\n",
    "df['date_column'] = pd.to_datetime(df['date_column'], errors='coerce')\n",
    "\n",
    "# Convert column to category\n",
    "df['category_column'] = df['category_column'].astype('category')\n",
    "```\n",
    "\n",
    "## 4. Handling Outliers\n",
    "\n",
    "Why: Handling outliers is important because outliers can skew your analysis and affect the accuracy of statistical measures and models. Properly managing outliers ensures that your data represents the true patterns and trends.\n",
    "\n",
    "When: You need to handle outliers when:\n",
    "\n",
    "Outliers are suspected to be errors or anomalies that do not reflect the true nature of the data.\n",
    "Outliers significantly affect the results of statistical analyses or machine learning models.\n",
    "You want to improve the robustness and accuracy of your analysis or model.\n",
    "\n",
    "``` python\n",
    "# Identify outliers using IQR method\n",
    "Q1 = df['column'].quantile(0.25)\n",
    "Q3 = df['column'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers = df[(df['column'] < lower_bound) | (df['column'] > upper_bound)]\n",
    "\n",
    "# Remove outliers\n",
    "df_cleaned = df[(df['column'] >= lower_bound) & (df['column'] <= upper_bound)]\n",
    "\n",
    "# Cap outliers (winsorization)\n",
    "df['column'] = np.clip(df['column'], lower_bound, upper_bound)\n",
    "```\n",
    "\n",
    "## 5. String Cleaning\n",
    "\n",
    "Why: String cleaning ensures that text data is consistent, free from errors, and in a format that can be easily analyzed or used in further processing. Cleaning text helps improve data quality and accuracy in analyses, searches, and other operations.\n",
    "\n",
    "When:\n",
    "\n",
    "Text data contains unnecessary whitespace, inconsistencies in case, or unwanted characters that can affect analysis or processing.\n",
    "You want to standardize text data for comparison, searching, or further manipulation.\n",
    "\n",
    "``` python\n",
    "# Strip whitespace\n",
    "df['text_column'] = df['text_column'].str.strip()\n",
    "\n",
    "# Convert to lowercase\n",
    "df['text_column'] = df['text_column'].str.lower()\n",
    "\n",
    "# Replace specific strings\n",
    "df['text_column'] = df['text_column'].str.replace('old', 'new')\n",
    "\n",
    "# Remove special characters\n",
    "df['text_column'] = df['text_column'].str.replace('[^\\w\\s]', '')\n",
    "```\n",
    "\n",
    "## 6. Standardizing Values\n",
    "\n",
    "Why: Standardizing values ensures consistency across your dataset. It helps in making data easier to compare, analyze, and interpret by ensuring that similar data entries are represented in the same format or terminology.\n",
    "\n",
    "When:\n",
    "Data entries use different formats or terminologies that should be unified.\n",
    "Consistency is needed for accurate comparisons, analysis, and reporting.\n",
    "\n",
    "``` python\n",
    "# Standardize categorical values\n",
    "df['category'] = df['category'].replace({'Yr': 'Year', 'Mth': 'Month'})\n",
    "\n",
    "# Standardize date formats\n",
    "df['date'] = pd.to_datetime(df['date']).dt.strftime('%Y-%m-%d')\n",
    "```\n",
    "\n",
    "## 7. Handling Inconsistent Capitalization\n",
    "\n",
    "Why: Handling inconsistent capitalization ensures uniformity in text data, making it easier to analyze, compare, and process. Consistent capitalization helps in avoiding issues related to case sensitivity, which can affect sorting, searching, and matching operations.\n",
    "\n",
    "When: You need to handle inconsistent capitalization when:\n",
    "\n",
    "Text data has mixed cases that should be standardized for consistent processing.\n",
    "Consistent casing is required for accurate analysis or comparison.\n",
    "\n",
    "``` python\n",
    "# Title case\n",
    "df['name'] = df['name'].str.title()\n",
    "\n",
    "# Upper case\n",
    "df['code'] = df['code'].str.upper()\n",
    "```\n",
    "\n",
    "## 8. Binning Numerical Data\n",
    "Why: Binning helps in simplifying numerical data by grouping continuous values into discrete categories or intervals. This can make data analysis easier, especially for summarizing, visualizing, or analyzing data at a higher level.\n",
    "\n",
    "When: You need to bin numerical data when:\n",
    "\n",
    "You want to categorize continuous variables into distinct groups to simplify analysis or reporting.\n",
    "You are interested in summarizing data into ranges or segments for better insights or comparisons.\n",
    "\n",
    "``` python\n",
    "# Create bins\n",
    "df['age_group'] = pd.cut(df['age'], bins=[0, 18, 35, 50, 65, 100], labels=['0-18', '19-35', '36-50', '51-65', '65+'])\n",
    "```\n",
    "\n",
    "## 9. One-Hot Encoding\n",
    "Why: One-hot encoding is used to convert categorical variables into a numerical format that can be used in machine learning models. By creating binary columns for each category, it allows models to understand and process categorical data effectively without assuming any ordinal relationship between the categories.\n",
    "\n",
    "When:\n",
    "\n",
    "You have categorical variables that need to be included in a machine learning model.\n",
    "The categories do not have an inherent order, and you want to avoid introducing a misleading ordinal relationship.\n",
    "You want to convert categorical data into a format that is compatible with algorithms that require numerical input.\n",
    "``` python\n",
    "# One-hot encode categorical variables\n",
    "df_encoded = pd.get_dummies(df, columns=['category_column'])\n",
    "```\n",
    "\n",
    "## 10. Handling Skewed Data\n",
    "\n",
    "Why: Skewed data is when your data isn't evenly distributed, with a lot of values on one side of the scale. This can mess with machine learning models because they often assume that data is more balanced. Transforming skewed data helps make it more normal (balanced), so the model can work better.\n",
    "\n",
    "When:\n",
    "\n",
    "You have a column in your dataset that has a lot of extreme values on one end (skewed).\n",
    "You need to make your data more balanced for the machine learning model to understand it better.\n",
    "\n",
    "``` python\n",
    "from scipy import stats\n",
    "\n",
    "# Log transformation  This squashes large values and spreads out smaller values, making the data more balanced.\n",
    "df['log_column'] = np.log1p(df['skewed_column'])\n",
    "\n",
    "# Box-Cox transformation\n",
    "df['boxcox_column'], _ = stats.boxcox(df['skewed_column'])\n",
    "```\n",
    "\n",
    "## 11. Scaling Numerical Features\n",
    "\n",
    "Why: Features in your dataset might have different ranges (e.g., one feature ranges from 1 to 10, while another ranges from 100 to 1000). This difference can confuse some machine learning algorithms, especially those that rely on distance or gradient. Scaling makes sure all features are on the same playing field, helping the algorithm perform better.\n",
    "\n",
    "When:\n",
    "\n",
    "Your dataset has numerical features with varying units or scales.\n",
    "You want to ensure that the model gives equal importance to all features, especially in models sensitive to scale.\n",
    "``` python\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# Standardization (mean=0, std=1)\n",
    "scaler = StandardScaler()\n",
    "df['scaled_column'] = scaler.fit_transform(df[['column']])\n",
    "\n",
    "# Min-Max scaling (0 to 1 range)\n",
    "min_max_scaler = MinMaxScaler()\n",
    "df['normalized_column'] = min_max_scaler.fit_transform(df[['column']])\n",
    "```\n",
    "\n",
    "## 12. Handling Imbalanced Data\n",
    "\n",
    "Why: Handling imbalanced data is crucial in machine learning to ensure that models do not become biased towards the majority class. An imbalanced dataset can lead to poor performance, especially in predicting the minority class, as the model may learn to favor the majority class.\n",
    "\n",
    "When:\n",
    "\n",
    "Our dataset has a significant imbalance between the classes, meaning one class occurs much more frequently than the other.\n",
    "You want to improve the model's performance on the minority class, especially in classification tasks where both classes are of interest.\n",
    "\n",
    "``` python\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Oversample minority class using SMOTE\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "```\n",
    "\n",
    "## 13. Merging and Concatenating DataFrames\n",
    "\n",
    "Why: Merging and concatenating DataFrames are essential operations for combining datasets to create a unified dataset from multiple sources. These methods help in integrating related data and managing larger datasets efficiently.\n",
    "\n",
    "When: You need to merge or concatenate DataFrames when:\n",
    "\n",
    "You want to combine data from different sources or tables based on a common key or index.\n",
    "You need to stack multiple datasets together to perform comprehensive analysis or to prepare data for further processing.\n",
    "\n",
    "``` python\n",
    "# Merge two DataFrames\n",
    "merged_df = pd.merge(df1, df2, on='key_column', how='inner')\n",
    "\n",
    "# Concatenate DataFrames\n",
    "concatenated_df = pd.concat([df1, df2], axis=0, ignore_index=True)\n",
    "```\n",
    "\n",
    "## 14. Reshaping Data\n",
    "Why: Reshaping data is like reorganizing your data to fit it into a structure that makes it easier to analyze or visualize. This process helps you better understand the data, make it compatible with tools for analysis, or prepare it for reporting. It's especially useful when you need to look at the data from different angles or transform it into a format that meets specific needs.\n",
    "\n",
    "When: You might need to reshape your data in situations like:\n",
    "\n",
    "Pivoting: When you want to summarize data by converting rows into columns, making it easier to spot patterns.\n",
    "Unpivoting (Melting): When you need to convert wide data into a longer format, especially when you have multiple categories that you want to compare side by side.\\\\\n",
    "\n",
    "``` python\n",
    "# Pivot table   Reshapes the data so that 'category' values become columns, and 'value' becomes the data in these columns.\n",
    "pivot_df = df.pivot(index='date', columns='category', values='value')\n",
    "\n",
    "# Melt DataFrame\n",
    "melted_df = pd.melt(df, id_vars=['date'], value_vars=['category1', 'category2'])\n",
    "```\n",
    "\n",
    "## 15. Handling Time Series Data\n",
    "Why: Handling time series data involves managing and analyzing data that is indexed by time, which is crucial for time-based analysis, forecasting, and trend identification. Proper handling ensures that time series data is organized, resampled, and converted correctly for accurate analysis.\n",
    "\n",
    "When: You need to handle time series data when:\n",
    "\n",
    "Your dataset involves data points indexed by time, and you need to perform time-based analysis or visualization.\n",
    "You need to aggregate, resample, or convert time zones for accurate analysis and interpretation of time series data.\n",
    "\n",
    "``` python\n",
    "# Set date as index\n",
    "df.set_index('date', inplace=True)\n",
    "\n",
    "# Resample time series data\n",
    "daily_data = df.resample('D').mean()\n",
    "\n",
    "# Handle timezone conversions\n",
    "df['date'] = df['date'].dt.tz_localize('UTC').dt.tz_convert('US/Eastern')\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
